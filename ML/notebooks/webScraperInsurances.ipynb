{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lang</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Lang, Text, Username, Name, Location, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/twitterInsuranceData.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_scrape(keyword, count, cls):\n",
    "    url = 'http://127.0.0.1:8000/tweets/'\n",
    "    obj = {\n",
    "        \"keyword\": keyword,\n",
    "        \"count\": count\n",
    "        }\n",
    "\n",
    "    r = requests.post(url, json=obj)\n",
    "    data = r.json()\n",
    "    tweets = data['tweets']\n",
    "    for tweet in tweets:\n",
    "        temp = [tweet['lang'], tweet['full_text'], tweet['user']['screen_name'], tweet['user']['name'], tweet['user']['location'], cls]\n",
    "        df.loc[len(df.index)] = temp\n",
    "    \n",
    "    return len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1444"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_scrape('motor insurance', 250, 'motor')\n",
    "twitter_scrape('life insurance', 250, 'life')\n",
    "twitter_scrape('home insurance', 250, 'home')\n",
    "twitter_scrape('health insurance', 250, 'health')\n",
    "twitter_scrape('travel insurance', 250, 'travel')\n",
    "twitter_scrape('birth insurance', 100, 'life')\n",
    "twitter_scrape('accident insurance', 100, 'motor')\n",
    "twitter_scrape('death insurance', 100, 'life')\n",
    "twitter_scrape('love traveling India', 100, 'travel')\n",
    "twitter_scrape('mountains india', 100, 'travel')\n",
    "twitter_scrape('india tourism', 100, 'travel')\n",
    "twitter_scrape('medical emergency', 100, 'health')\n",
    "twitter_scrape('hospital bills', 100, 'health')\n",
    "twitter_scrape('health issues', 100, 'health')\n",
    "twitter_scrape('suffering from terminal disease', 100, 'health')\n",
    "twitter_scrape('earthquake damage', 50, 'home')\n",
    "twitter_scrape('natural calamity', 50, 'home')\n",
    "twitter_scrape('flood damage', 50, 'home')\n",
    "twitter_scrape('landslide damage', 50, 'home')\n",
    "twitter_scrape('house theft', 100, 'home')\n",
    "twitter_scrape('car accident', 150, 'motor')\n",
    "twitter_scrape('motorcycle accident', 150, 'motor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1381, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "for i in range(len(df)):\n",
    "    txt = re.sub('[^a-zA-Z]', ' ', df['Text'][i])\n",
    "    txt = txt.lower()\n",
    "    txt = txt.split()\n",
    "    wl = WordNetLemmatizer()\n",
    "    txt = [wl.lemmatize(word) for word in txt if not word in set(stopwords.words('english'))]\n",
    "    txt = ' '.join(txt)\n",
    "    df['Text'][i] = txt\n",
    "\n",
    "new_df = df[df['Lang'] == 'en']\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('../data/twitterInsuranceData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lang</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>lionelwright never seen kenyan president going...</td>\n",
       "      <td>muema626</td>\n",
       "      <td>Jack the KING</td>\n",
       "      <td>Nairobi</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>rt lqlana due increased extreme volatile clima...</td>\n",
       "      <td>lafantome</td>\n",
       "      <td>fantome</td>\n",
       "      <td>Central Texas</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>rt obamamojang think dhalsim say yoga day day ...</td>\n",
       "      <td>AegisPancakes</td>\n",
       "      <td>Brandon ðŸ¦”</td>\n",
       "      <td>Destiny Islands</td>\n",
       "      <td>motor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>birth control free insurance condom free insur...</td>\n",
       "      <td>bgmtiara</td>\n",
       "      <td>TheBossBran ðŸ©µ</td>\n",
       "      <td>St Louis, MO</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>en</td>\n",
       "      <td>motoman mailbox property worth insurance car e...</td>\n",
       "      <td>Courtne08241586</td>\n",
       "      <td>ðŸ¦‹Blue AngelðŸ¦‹</td>\n",
       "      <td></td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Lang                                               Text         Username  \\\n",
       "0   en  lionelwright never seen kenyan president going...         muema626   \n",
       "1   en  rt lqlana due increased extreme volatile clima...        lafantome   \n",
       "2   en  rt obamamojang think dhalsim say yoga day day ...    AegisPancakes   \n",
       "4   en  birth control free insurance condom free insur...         bgmtiara   \n",
       "5   en  motoman mailbox property worth insurance car e...  Courtne08241586   \n",
       "\n",
       "            Name         Location   label  \n",
       "0  Jack the KING          Nairobi  health  \n",
       "1        fantome    Central Texas    home  \n",
       "2      Brandon ðŸ¦”  Destiny Islands   motor  \n",
       "4  TheBossBran ðŸ©µ     St Louis, MO    life  \n",
       "5   ðŸ¦‹Blue AngelðŸ¦‹                     life  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
